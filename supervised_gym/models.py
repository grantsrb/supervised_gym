import numpy as np
import torch
import torch.nn as nn
from supervised_gym.utils.torch_modules import Flatten, Reshape, GaussianNoise
from supervised_gym.utils.utils import update_shape
# update_shape(shape, kernel=3, padding=0, stride=1, op="conv"):

class Model(torch.nn.Module):
    """
    This is the base class for all models within this project. It
    ensures the appropriate members are added to the model.

    All models that inherit from Model must implement a step function
    that takes a float tensor of dims (B, C, H, W)
    """
    def __init__(self,
        inpt_shape,
        actn_size,
        h_size=128,
        bnorm=False,
        conv_drop=0,
        dense_drop=0,
        conv_noise=0,
        dense_noise=0,
        *args, **kwargs
    ):
        """
        Args: 
            inpt_shape: tuple or listlike (..., C, H, W)
                the shape of the input
            actn_size: int
                the number of potential actions
            h_size: int
                the size of the hidden dimension for the dense layers
            bnorm: bool
                if true, the model uses batch normalization
            conv_drop: float [0 to 1 inclusive]
                probability of a neuron being dropped out in the
                convolutions of the network
            dense_drop: float [0 to 1 inclusive]
                probability of a neuron being dropped out in the
                dense layers of the network
            conv_noise: float
                standard deviation of noise added to the neurons at
                each convolutional layer
            dense_noise: float
                standard deviation of noise added to the neurons at
                each dense layer
        """
        super().__init__()
        self.inpt_shape = inpt_shape
        self.actn_size = actn_size
        self.h_size = h_size
        self.bnorm = bnorm
        self.conv_drop = conv_drop
        self.dense_drop = dense_drop
        self.conv_noise = conv_noise
        self.dense_noise = dense_noise

    @property
    def is_cuda(self):
        try:
            return next(self.parameters()).is_cuda
        except:
            return False

    def get_device(self):
        try:
            return next(self.parameters()).get_device()
        except:
            return False

    def reset(self, batch_size):
        """
        Only necessary to override if building a recurrent network.
        This function should reset any recurrent state in a model.

        Args:
            batch_size: int
                the size of the incoming batches
        """
        pass

    def reset_to_step(self, step=1):
        """
        Only necessary to override if building a recurrent network.
        This function resets all recurrent states in a model to the
        recurrent state that occurred after the first step in the last
        call to forward.

        Args:
            step: int
                the index + 1 of the step to revert the recurrence to
        """
        pass

    def step(self, x):
        """
        Performs a single step rather than a complete sequence of steps

        Args:
            x: torch FloatTensor (B, C, H, W)
        Returns:
            pred: torch Float Tensor (B, K)
        """
        pass

    def forward(self, x):
        """
        Performs multiple steps in time rather than a single step.

        Args:
            x: torch FloatTensor (B, S, C, H, W)
        Returns:
            pred: torch Float Tensor (B, S, K)
        """
        pass

class RandomModel(Model):
    def __init__(self, *args, **kwargs):
        super().__init__(inpt_shape=None, **kwargs)

    def forward(self, x, dones=None):
        """
        Args:
            x: torch Float Tensor (B, S, C, H, W)
            dones: torch LongTensor (B, S)
        """
        if len(x.shape) == 4:
            return self.step(x)
        else:
            actn = torch.zeros(*x.shape[:2], self.actn_size).float()
            rand = torch.randint(
                low=0,
                high=self.actn_size,
                size=(int(np.prod(x.shape[:2])),)
            )
            actn = actn.reshape(int(np.prod(x.shape[:2])), -1)
            actn[torch.arange(len(actn)).long(), rand] = 1
            if x.is_cuda: actn.cuda()
            return actn

    def step(self, x):
        """
        Args:
            x: torch Float Tensor (B, C, H, W)
        """
        rand = torch.randint(
            low=0,
            high=self.actn_size,
            size=(len(x),)
        )
        actn = torch.zeros(len(x), self.actn_size).float()
        actn[torch.arange(len(x)).long(), rand] = 1
        if x.is_cuda: actn.cuda()
        return actn

class SimpleCNN(Model):
    """
    A simple convolutional network with no recurrence.
        conv2d
        bnorm
        relu
        conv2d
        bnorm
        relu
        linear
        bnorm
        relu
        linear
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.depths = [self.inpt_shape[-3], 32, 48]
        self.kernels = [3, 3]
        self.strides = [1, 1]
        self.paddings = [0, 0]
        modules = []
        shape = [*self.inpt_shape[-3:]]
        self.shapes = [shape]
        for i in range(len(self.depths)-1):
            kernel = [self.kernels[i], self.kernels[i]]
            if shape[-2] < kernel[0]:
                kernel[0] = int(shape[-2])
            if shape[-1] < kernel[1]:
                kernel[1] = int(shape[-1])
            # CONV
            modules.append(
                nn.Conv2d(
                    self.depths[i],
                    self.depths[i+1],
                    kernel_size=kernel,
                    stride=self.strides[i],
                    padding=self.paddings[i]
                )
            )
            if self.conv_drop > 0:
                modules.append(nn.Dropout(self.conv_drop))
            # RELU
            modules.append(GaussianNoise(self.conv_noise))
            modules.append(nn.ReLU())
            # Batch Norm
            if self.bnorm:
                modules.append(nn.BatchNorm2d(self.depths[i+1]))
            # Track Activation Shape Change
            shape = update_shape(
                shape, 
                depth=self.depths[i+1],
                kernel=kernel,
                stride=self.strides[i],
                padding=self.paddings[i]
            )
            self.shapes.append(shape)
        self.features = nn.Sequential(*modules)

        # Make Output MLP
        self.flat_size = int(np.prod(shape))
        modules = [
            Flatten(),
            nn.Linear(self.flat_size, self.h_size),
            GaussianNoise(self.dense_noise),
            nn.ReLU()
        ]
        if self.dense_drop > 0:
            modules.append(nn.Dropout(self.dense_drop))
        if self.bnorm:
            modules.append(nn.BatchNorm1d(self.h_size))
        self.dense = nn.Sequential(
            *modules,
            nn.Linear(self.h_size, self.actn_size)
        )
        # Full Model All Together
        self.full_model = nn.Sequential(
            self.features,
            self.dense
        )

    def step(self, x, *args, **kwargs):
        """
        Performs a single step rather than a complete sequence of steps

        Args:
            x: torch FloatTensor (B, C, H, W)
        Returns:
            pred: torch Float Tensor (B, K)
        """
        return self.full_model(x)

    def forward(self, x, *args, **kwargs):
        """
        Args:
            x: torch FloatTensor (B, S, C, H, W)
        Returns:
            actns: torch FloatTensor (B, S, N)
                N is equivalent to self.actn_size
        """
        b,s = x.shape[:2]
        fx = self.full_model(x.reshape(-1, *x.shape[2:]))
        return fx.reshape(b,s,-1)

class SimpleLSTM(Model):
    """
    A recurrent LSTM model.
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        assert self.bnorm == False, "bnorm must be False. it does not work with Recurrence!"

        # Convs
        cnn = SimpleCNN(*args, **kwargs)
        self.shapes = cnn.shapes
        self.features = cnn.features

        # LSTM
        self.flat_size = cnn.flat_size
        self.lstm = nn.LSTMCell(self.flat_size, self.h_size)

        # Dense
        modules = []
        if self.dense_drop > 0:
            modules.append(nn.Dropout(self.dense_drop))
        modules.append(GaussianNoise(self.dense_noise))
        modules.append(nn.ReLU())
        modules.append(nn.Linear(self.h_size, self.actn_size))
        self.dense = nn.Sequential(*modules)

        # Memory
        self.h = None
        self.c = None
        self.reset(batch_size=1)

    def reset(self, batch_size=1):
        """
        Resets the memory vectors

        Args:
            batch_size: int
                the size of the incoming batches
        Returns:
            None
        """
        self.h = torch.zeros(batch_size, self.h_size).float()
        self.c = torch.zeros(batch_size, self.h_size).float()
        # Ensure memory is on appropriate device
        if self.features[0].weight.is_cuda:
            self.h.to(self.get_device())
            self.c.to(self.get_device())
        self.prev_hs = [self.h]
        self.prev_cs = [self.c]

    def partial_reset(self, dones):
        """
        Uses the done signals to reset appropriate parts of the h and
        c vectors.

        Args:
            dones: torch LongTensor (B,)
                h and c are zeroed along any row in which dones[row]==1
        Returns:
            h: torch FloatTensor (B, H)
            c: torch FloatTensor (B, H)
        """
        mask = (1-dones).unsqueeze(-1)
        h = self.h*mask
        c = self.c*mask
        return h,c

    def reset_to_step(self, step=1):
        """
        Only necessary to override if building a recurrent network.
        This function resets all recurrent states in a model to the
        recurrent state that occurred after the first step in the last
        call to forward.

        Args:
            step: int
                the index + 1 of the step to revert the recurrence to
        """
        assert (step-1) < len(self.prev_hs) and (step-1) >= 0, "invalid step"
        self.h = self.prev_hs[step-1].detach().data
        self.c = self.prev_cs[step-1].detach().data
        if self.is_cuda:
            self.h.to(self.get_device())
            self.c.to(self.get_device())

    def step(self, x, *args, **kwargs):
        """
        Performs a single step rather than a complete sequence of steps

        Args:
            x: torch FloatTensor (B, C, H, W)
        Returns:
            pred: torch Float Tensor (B, K)
        """
        if x.is_cuda:
            self.h = self.h.to(x.get_device())
            self.c = self.c.to(x.get_device())
        fx = self.features(x)
        fx = fx.reshape(len(x), -1) # (B, N)
        self.h, self.c = self.lstm(fx, (self.h, self.c))
        return self.dense(self.h)

    def forward(self, x, dones, *args, **kwargs):
        """
        Args:
            x: torch FloatTensor (B, S, C, H, W)
            dones: torch Long Tensor (B, S)
                the done signals for the environment. the h and c
                vectors are reset when encountering a done signal
        Returns:
            actns: torch FloatTensor (B, S, N)
                N is equivalent to self.actn_size
        """
        seq_len = x.shape[1]
        outputs = []
        self.prev_hs = []
        self.prev_cs = []
        if x.is_cuda:
            dones = dones.to(x.get_device())
        for s in range(seq_len):
            preds = self.step(x[:,s])
            outputs.append(preds.unsqueeze(1))
            self.h, self.c = self.partial_reset(dones[:,s])
            self.prev_hs.append(self.h.detach().data)
            self.prev_cs.append(self.c.detach().data)
        return torch.cat(outputs, dim=1)




